{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03a978673c794a728239a0f22fb02efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4e852ac00b940929da8aab3ce7bf829",
              "IPY_MODEL_18eca0d9711b47829e0e06209c09dd31",
              "IPY_MODEL_197f95364def46e5bf602a64b4bdd3a8"
            ],
            "layout": "IPY_MODEL_bf1f0fefce4242e9ad33fa79434b9ec8"
          }
        },
        "f4e852ac00b940929da8aab3ce7bf829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d53aef12642c4fd29d69a7045306b5e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_58fa431de98d4974b66f59784bfa8f3b",
            "value": "Downloading‚Äáhttps://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:‚Äá"
          }
        },
        "18eca0d9711b47829e0e06209c09dd31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdc20adca3094a478d6b4eea1eee7a45",
            "max": 52452,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3544231f8694c5388816d63ecfa6c8f",
            "value": 52452
          }
        },
        "197f95364def46e5bf602a64b4bdd3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b5acb50c4bc469db80aa2e29c9b9c1d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8e8ae31a68b44717a4061da8d32f2cbb",
            "value": "‚Äá424k/?‚Äá[00:00&lt;00:00,‚Äá25.2MB/s]"
          }
        },
        "bf1f0fefce4242e9ad33fa79434b9ec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d53aef12642c4fd29d69a7045306b5e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58fa431de98d4974b66f59784bfa8f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdc20adca3094a478d6b4eea1eee7a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3544231f8694c5388816d63ecfa6c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b5acb50c4bc469db80aa2e29c9b9c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8ae31a68b44717a4061da8d32f2cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06e023a61b2b4a8795d4eb894213a6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a4cc94510ce4cca9f307f5d45430a64",
              "IPY_MODEL_50cbaeee732445a7adcb0697d9e2749c",
              "IPY_MODEL_6c8467dce3454e2cb9ed83d99a0493f7"
            ],
            "layout": "IPY_MODEL_659fa7b2b05c4688a90f2b306e2cdd45"
          }
        },
        "2a4cc94510ce4cca9f307f5d45430a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_772af2386f304432b1d8756b4661a2e5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_790f944522c44e0e8e8caf5513e402db",
            "value": "Downloading‚Äáhttps://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:‚Äá"
          }
        },
        "50cbaeee732445a7adcb0697d9e2749c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c11c3ece0ddf464789c30f04a0e8da4f",
            "max": 52452,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9535bf95bc784228bfb62d90f42faba3",
            "value": 52452
          }
        },
        "6c8467dce3454e2cb9ed83d99a0493f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa40fad1ab9e4daf865bcd3d63df4cb0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7af11ea227e1489db47585c027acd418",
            "value": "‚Äá424k/?‚Äá[00:00&lt;00:00,‚Äá16.5MB/s]"
          }
        },
        "659fa7b2b05c4688a90f2b306e2cdd45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "772af2386f304432b1d8756b4661a2e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "790f944522c44e0e8e8caf5513e402db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c11c3ece0ddf464789c30f04a0e8da4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9535bf95bc784228bfb62d90f42faba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa40fad1ab9e4daf865bcd3d63df4cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7af11ea227e1489db47585c027acd418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AinSern/Natural-Language-Processing-NLP---01/blob/main/Assignment_1_1_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ASSIGNMENT 1**\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "---\n",
        "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
        "* Insert a text block below your code to briefly explain it, mentioning any libraries or functions utilized. Answer the questions in brief with examples.\n",
        "\n",
        "* Submit  \n",
        "1. The IPython Notebook (ipynb) file.  \n",
        "2. A PDF version of the notebook (converted from ipynb).\n",
        "3. The similarity score should be less than 15%"
      ],
      "metadata": {
        "id": "wknRrNHwjodK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 1: Tokenization & & Sentence Segmentation(20%)**\n",
        "(refer to the spacy tokenization concept which is explained after Question1 in activity-1)\n",
        "\n",
        "**Question -1:**\n",
        "\n",
        "##How can incorporating contextual information beyond single words enhance tokenization and improve performance in downstream tasks such as machine translation or question answering?"
      ],
      "metadata": {
        "id": "-0MGUnuLnXue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorporating contextual clues beyond individual words can enhance tokenization by addressing ambiguities in word boundaries, contractions, and multi-word units. For instance:\n",
        "\n",
        "1.Contractions: Splitting terms like \"can‚Äôt\" into \"can\" and \"not\" ensures semantic clarity.\n",
        "\n",
        "2.Multi-word units: Phrases such as \"New York\" may need to be tokenized as a single unit in geographic contexts.\n",
        "\n",
        "3.Named entities: Compound terms like \"Mr. Smith\" should remain intact to preserve entity meaning.\n",
        "\n",
        "4.Context-aware tokenization improves performance in downstream applications, including:\n",
        "\n",
        "5.Machine Translation: Accurate splitting prevents mistranslations (e.g., distinguishing \"I‚Äôm\" from \"Im\" as separate tokens).\n",
        "\n",
        "6.Question Answering: Identifying unified concepts (e.g., \"climate change\") aids in retrieving contextually relevant responses.\n",
        "\n",
        "This approach highlights how contextual sensitivity during tokenization contributes to more robust language processing workflows."
      ],
      "metadata": {
        "id": "T9jooJ_D1UQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Question 2**\n",
        "##Develop a tokenizer that considers contractions like \"can't\" and \"doesn't\", splitting them into their constituent words."
      ],
      "metadata": {
        "id": "VIl7PgRaeGjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I wouldn't've guessed that she'd leave early, but it doesn't seem surprising now. They're not coming either, are they?.\"\n",
        "#CODE HERE\n",
        "import re\n",
        "\n",
        "def tokenize_contractions(text):\n",
        "    # Define a regex pattern to match contractions\n",
        "    contractions_pattern = re.compile(r\"(\\w+)(n't|'ve|'ll|'d|'re|'s|'m)\")\n",
        "    # Replace contractions with their expanded forms\n",
        "    expanded_text = contractions_pattern.sub(r\"\\1 \\2\", text)\n",
        "    return expanded_text.split()\n",
        "\n",
        "text = \"I wouldn't've guessed that she'd leave early, but it doesn't seem surprising now. They're not coming either, are they?\"\n",
        "tokens = tokenize_contractions(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "aQ_rfINQfbA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5edc377d-3f74-4b29-cd99-836d5fc82d84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'would', \"n't've\", 'guessed', 'that', 'she', \"'d\", 'leave', 'early,', 'but', 'it', 'does', \"n't\", 'seem', 'surprising', 'now.', 'They', \"'re\", 'not', 'coming', 'either,', 'are', 'they?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 3:**\n",
        "##Implement a Python script to remove Twitter username handles from a given twitter text.\n",
        "\n"
      ],
      "metadata": {
        "id": "1BRekASOnmsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Awesome brainstorming session with @AlexM and @SamanthaT! Excited for what‚Äôs next. Appreciate the valuable input @DevMaster üí°üöÄ #collaboration #growth\"\n",
        "#CODE HERE\n",
        "import re\n",
        "\n",
        "def remove_twitter_handles(text):\n",
        "    # Regex pattern to match Twitter handles\n",
        "    pattern = r'@\\w+'\n",
        "    # Replace handles with an empty string\n",
        "    cleaned_text = re.sub(pattern, '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "text = \"Awesome brainstorming session with @AlexM and @SamanthaT! Excited for what‚Äôs next. Appreciate the valuable input @DevMaster üí°üöÄ #collaboration #growth\"\n",
        "cleaned_text = remove_twitter_handles(text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "-MD-tQZKsCkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c039e0-68c4-402f-b8af-77c2a20fa64d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Awesome brainstorming session with  and ! Excited for what‚Äôs next. Appreciate the valuable input  üí°üöÄ #collaboration #growth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:**\n",
        "\n",
        "##In many NLP tasks, especially tasks involving large documents, it's important to not just tokenize words but also segment sentences. Write a Python script to segment a multi-paragraph text into individual sentences, considering the following cases:\n",
        "\n",
        "Sentences ending with abbreviations like \"Dr.\", \"Mr.\", etc.\n",
        "Sentences that contain dialogue within quotation marks.\n",
        "Use spaCy or NLTK for sentence segmentation and briefly explain how contextual awareness impacts this process."
      ],
      "metadata": {
        "id": "PyRZ4X3Aucy1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvGnXMtpx0mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Input_example_text ='Dr. Adams, a renowned scientist, stated, \"We\\'ll analyze the samples by 3 p.m. tomorrow."
      ],
      "metadata": {
        "id": "fv3DDOgFuhbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code here\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def segment_sentences(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "    return sentences\n",
        "\n",
        "input_text = 'Dr. Adams, a renowned scientist, stated, \"We\\'ll analyze the samples by 3 p.m. tomorrow.\"'\n",
        "sentences = segment_sentences(input_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "87IxZLTXukA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba407fa-80f5-40d1-9067-68e48b2bbde3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dr. Adams, a renowned scientist, stated, \"We\\'ll analyze the samples by 3 p.m. tomorrow.\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 2.  - Regular Expressions (30%)**"
      ],
      "metadata": {
        "id": "zME8pbf7KWZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Regular Expressions**\n",
        "A regular expression, often abbreviated as regex, is a powerful and flexible tool for pattern matching and text manipulation. It consists of a sequence of characters that defines a search pattern, allowing you to perform various text-related tasks such as text validation, data extraction, text cleaning, and more. Regular expressions are used in programming languages and text editors and are constructed using a combination of regular characters, special characters, and metacharacters to specify search criteria. Learning to use regular expressions effectively can greatly enhance text processing tasks, making them a valuable skill in fields like natural language processing, data extraction, and data validation."
      ],
      "metadata": {
        "id": "tRR6kMx1haKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python includes a builtin module called `re` which provides regular expression matching operations (Click [here](https://docs.python.org/3/library/re.html) for the official module documentation). Once the module is imported into your code, you can use all of the available capabilities for performing pattern-based matching or searching using regular expressions."
      ],
      "metadata": {
        "id": "hhp7XiXXh-CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##**Question - 1**\n",
        "\n",
        "##Write a Python script that uses regular expressions to extract all email addresses and URLs from a text. Ensure your script handles:\n",
        "\n",
        "##Multiple TLDs (e.g., .com, .org)\n",
        "##Edge cases like emails with numbers (e.g., john123@domain.com)"
      ],
      "metadata": {
        "id": "ojyfJF7bymRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Input_text = \"For more details, contact us at support@myemail.com or visit https://example.org. Alternatively, you can email sales@company123.org.\""
      ],
      "metadata": {
        "id": "evyfHbBrveNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "import re\n",
        "\n",
        "def extract_emails_and_urls(text):\n",
        "    # Regex pattern for emails\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    # Regex pattern for URLs\n",
        "    url_pattern = r'https?://(?:www\\.)?\\S+'\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    return emails, urls\n",
        "\n",
        "input_text = \"For more details, contact us at support@myemail.com or visit https://example.org. Alternatively, you can email sales@company123.org.\"\n",
        "emails, urls = extract_emails_and_urls(input_text)\n",
        "print(\"Emails:\", emails)\n",
        "print(\"URLs:\", urls)"
      ],
      "metadata": {
        "id": "wlcFDE341Idg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc32a335-8e38-4fa5-9aa0-4f429da6b99a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emails: ['support@myemail.com', 'sales@company123.org']\n",
            "URLs: ['https://example.org.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**Question- 2**\n",
        "\n",
        "##Implement a Python program to find URLs in the given string using Regular expression."
      ],
      "metadata": {
        "id": "17YpjyzUo8oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text= '<p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a><a href=\"https://openai.com\">OpenAI Homepage</a><a href=\"https://docs.python.org\">Python Documentation</a>'\n",
        "##Your code here\n",
        "import re\n",
        "\n",
        "def extract_urls(text):\n",
        "    url_pattern = r'https?://(?:www\\.)?\\S+'\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    return urls\n",
        "\n",
        "text = '<p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a><a href=\"https://openai.com\">OpenAI Homepage</a><a href=\"https://docs.python.org\">Python Documentation</a>'\n",
        "urls = extract_urls(text)\n",
        "print(urls)"
      ],
      "metadata": {
        "id": "gRsGKmmskWJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f76d0c-e3e4-4b84-8ec8-cb611ebe2120"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://w3resource.com\">Python', 'http://github.com\">Even', 'https://openai.com\">OpenAI', 'https://docs.python.org\">Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using regular expressions based pattern matching on real world text**\n",
        "\n",
        "For the purposes of demonstration, here's a dummy paragraph of text. A few observations here:\n",
        "* The text has multiple paragraphs with each paragraph having more than one sentence.\n",
        "* Some of the words are capitalized (first letter is in uppercase followed by lowercase letters)."
      ],
      "metadata": {
        "id": "Uf1Pm9CHs1LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph is not going to be detected by either of the regex patterns below.\n",
        "\"\"\"\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPLUtLxVtiTH",
        "outputId": "59ca3969-3381-4a70-fd3d-a42e97902fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph is not going to be detected by either of the regex patterns below.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code block shows a regular expression that matches only those strings that:\n",
        "1. are at the start of a line and\n",
        "2. the string does not start with a number or a whitespace\n",
        "\n",
        "`re.findall()` finds all matches of the pattern in the text under consideration. The output is a list of strings that matched."
      ],
      "metadata": {
        "id": "zqmf8KSFtt2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further, the regular expression defined below matches the words that are capitalized."
      ],
      "metadata": {
        "id": "UyYWQ5Tzttzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##code block - 3\n",
        "re_pattern2 = r'[A-Z][a-z]+'\n",
        "print(re.findall(re_pattern2, text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxA3QOHHtqOg",
        "outputId": "84207169-f54a-4da7-8d7c-644e9f5b8f78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Contents', 'Python', 'Examples', 'Even', 'More', 'Examples', 'Open', 'Homepage', 'Python', 'Documentation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Following is a text excerpt on \"Inaugural Address\" taken from the website of the [Joint Congressional Committee on Inaugural Ceremonies](https://www.inaugural.senate.gov/inaugural-address/):"
      ],
      "metadata": {
        "id": "J0VUlW3RunCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inau_text=\"\"\"The custom of delivering an address on Inauguration Day started with the very first Inauguration‚ÄîGeorge Washington‚Äôs‚Äîon April 30, 1789(04-30-1789). ex:-18.5. After taking his oath of office on the balcony of Federal Hall in New York City, Washington proceeded to the Senate chamber where he read a speech before members of Congress and other dignitaries. His second Inauguration took place in Philadelphia on March 4, 1793(03/04/1793), in the Senate chamber of Congress Hall. There, Washington gave the shortest Inaugural address on record‚Äîjust 135 words ‚Äîbefore repeating the oath of office.\n",
        "Every President since Washington has delivered an Inaugural address. While many of the early Presidents read their addresses before taking the oath, current custom dictates that the Chief Justice of the Supreme Court administer the oath first, followed by the President‚Äôs speech.\n",
        "William Henry Harrison delivered the longest Inaugural address, at 8,445 words, on March 4, 1841‚Äîa bitterly cold, wet day. He died one month later of pneumonia, believed to have been brought on by prolonged exposure to the elements on his Inauguration Day. John Adams‚Äô Inaugural address, which totaled 2,308 words, contained the longest sentence, at 737 words. After Washington‚Äôs second Inaugural address, the next shortest was Franklin D. Roosevelt‚Äôs fourth address on January 20, 1945(01-20-1945), at just 559.0 words. Roosevelt had chosen to have a simple Inauguration at the White House in light of the nation‚Äôs involvement in World War II.\n",
        "In 1921, Warren G. Harding became the first President to take his oath and deliver his Inaugural address through loud speakers. In 1925, Calvin Coolidge‚Äôs Inaugural address was the first to be broadcast nationally by radio. And in 1949, Harry S. Truman became the first President to deliver his Inaugural address over television airwaves.\n",
        "Most Presidents use their Inaugural address to present their vision of America and to set forth their goals for the nation. Some of the most eloquent and powerful speeches are still quoted today. In 1865, in the waning days of the Civil War, Abraham Lincoln stated, ‚ÄúWith malice toward none, with charity for all, with firmness in the right as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nation‚Äôs wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.‚Äù In 1933, Franklin D. Roosevelt avowed, ‚Äúwe have nothing to fear but fear itself.‚Äù And in 1961, John F. Kennedy declared, ‚ÄúAnd so my fellow Americans: ask not what your country can do for you‚Äîask what you can do for your country.‚Äù\n",
        "Today, Presidents deliver their Inaugural address on the West Front of the Capitol, but this has not always been the case. Until Andrew Jackson‚Äôs first Inauguration in 1829, most Presidents spoke in either the House or Senate chambers. Jackson became the first President to take his oath of office and deliver his address on the East Front Portico of the U.S. Capitol in 1829. With few exceptions, the next 37.0 Inaugurations took place there, until 1981, when Ronald Reagan‚Äôs Swearing-In Ceremony and Inaugural address occurred on the West Front Terrace of the Capitol. The West Front has been used ever since. You should also need to extract the floating numbers such as -55.5, 20.8%, -3.0 using your regular expression\"\"\"\n"
      ],
      "metadata": {
        "id": "wRxR7M-NuFtR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to  above code block -3 for the following questions"
      ],
      "metadata": {
        "id": "0qMOPAWe3wnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**Questions-3.A**\n",
        "## Identify all the positive and neagtive numbers with type of both intergers,float  in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of such words in the text. Then, run the Python code snippet to automatically display the matched strings according to the pattern.*.\n",
        "\n",
        "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
      ],
      "metadata": {
        "id": "_Oap3GNEut1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Your code here\n",
        "import re\n",
        "\n",
        "def extract_numbers(text):\n",
        "    number_pattern = r'-?\\d+\\.?\\d*'\n",
        "    numbers = re.findall(number_pattern, text)\n",
        "    return numbers\n",
        "\n",
        "inau_text = \"\"\"The custom of delivering an address...\"\"\"  # Full text from the assignment\n",
        "numbers = extract_numbers(inau_text)\n",
        "print(numbers)"
      ],
      "metadata": {
        "id": "BNqyUNbPurJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ce184f-861e-498e-c43f-258809b5529b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation\n",
        "\n",
        "The regex pattern -?\\d+\\.?\\d* matches both positive and negative integers and floats in the context."
      ],
      "metadata": {
        "id": "IWQdvlG0dPKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question-3.B**\n",
        "##*Identify all the dates of all forms - text form(April 20, 1945) and digit form(xx-xx-xxxx, xx/xx/xxxx) in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of the dates in the text. Then, run the Python code snippet to automatically display a list of all such dates identified.*\n",
        "\n",
        "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
      ],
      "metadata": {
        "id": "-mkbLF4nu7la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Your code here\n",
        "import re\n",
        "\n",
        "def extract_dates(text):\n",
        "    date_pattern = r'\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
        "    dates = re.findall(date_pattern, text)\n",
        "    return dates\n",
        "\n",
        "dates = extract_dates(inau_text)\n",
        "print(dates)"
      ],
      "metadata": {
        "id": "2pxl72VhvEcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08c4427-4919-4b76-fec3-3767ef0046fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation\n",
        "The regex pattern matches dates in both text form (e.g., \"April 30, 1789\") and digit form (e.g., \"04-30-1789\")."
      ],
      "metadata": {
        "id": "TdUT4vfidQ_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 3: Lemmatization/Stemming(25%)**\n",
        "\n",
        "\n",
        "#**Question -1:**\n",
        "##How does the morphology of a language (e.g., agglutinative vs. fusional) impact the suitability of stemming vs. lemmatization?"
      ],
      "metadata": {
        "id": "QZSO1IrN06xI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here\n",
        "Agglutinative languages (e.g., Turkish, Finnish) add multiple suffixes to a root word, making stemming more effective as it chops off suffixes without considering context.\n",
        "\n",
        "Fusional languages (e.g., English, Spanish) change the form of a word to convey meaning, making lemmatization more suitable as it reduces words to their base form based on context."
      ],
      "metadata": {
        "id": "80ycUYp41Iy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        " ## **Question - 2 :**\n",
        "\n",
        "\n",
        "## Create a Python function that takes a sentence as input, performs lemmatization using **Stanza**, and removes stopwords from the lemmatized sentence.Return the cleaned and lemmatized sentence.\n",
        "\n",
        "Reference: https://github.com/stanfordnlp/stanza\n",
        "\n"
      ],
      "metadata": {
        "id": "EMMf51DdsaK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "!pip install stanza\n",
        "\n"
      ],
      "metadata": {
        "id": "udt8qI_viOXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953ffb33-3d49-44f8-8a1e-5aa8ab83845a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: stanza in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from stanza) (2.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (4.25.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (10.3.5.147)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-cusolver-cu12\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed nvidia-cusolver-cu12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BjF-pAck9nM",
        "outputId": "e764ef40-a471-45d7-d091-e03ae9abcd07"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-dyrIK8lp4X",
        "outputId": "b8410500-60f8-4a6a-8306-006a5d328047"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the necessary Stanza model and NLTK stopwords\n",
        "stanza.download('en')  # Download the English model for Stanza\n",
        "import nltk\n",
        "nltk.download('stopwords')  # Download NLTK stopwords\n",
        "\n",
        "def clean_and_lemmatize(sentence):\n",
        "    # Initialize the Stanza pipeline for English\n",
        "    nlp = stanza.Pipeline('en', processors='tokenize,lemma')\n",
        "\n",
        "    # Process the input sentence\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Get NLTK English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Extract lemmas and remove stopwords\n",
        "    lemmatized_words = []\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            lemma = word.lemma.lower()  # Convert lemma to lowercase\n",
        "            if lemma not in stop_words and lemma.isalpha():  # Keep only alphabetic tokens\n",
        "                lemmatized_words.append(lemma)\n",
        "\n",
        "    # Join the cleaned lemmas into a sentence\n",
        "    cleaned_sentence = ' '.join(lemmatized_words)\n",
        "    return cleaned_sentence\n",
        "\n",
        "# Example usage\n",
        "input_sentence = \"The quick brown fox jumps over the lazy dog while chasing a butterfly.\"\n",
        "output_sentence = clean_and_lemmatize(input_sentence)\n",
        "print(output_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519,
          "referenced_widgets": [
            "03a978673c794a728239a0f22fb02efb",
            "f4e852ac00b940929da8aab3ce7bf829",
            "18eca0d9711b47829e0e06209c09dd31",
            "197f95364def46e5bf602a64b4bdd3a8",
            "bf1f0fefce4242e9ad33fa79434b9ec8",
            "d53aef12642c4fd29d69a7045306b5e6",
            "58fa431de98d4974b66f59784bfa8f3b",
            "fdc20adca3094a478d6b4eea1eee7a45",
            "b3544231f8694c5388816d63ecfa6c8f",
            "7b5acb50c4bc469db80aa2e29c9b9c1d",
            "8e8ae31a68b44717a4061da8d32f2cbb",
            "06e023a61b2b4a8795d4eb894213a6c6",
            "2a4cc94510ce4cca9f307f5d45430a64",
            "50cbaeee732445a7adcb0697d9e2749c",
            "6c8467dce3454e2cb9ed83d99a0493f7",
            "659fa7b2b05c4688a90f2b306e2cdd45",
            "772af2386f304432b1d8756b4661a2e5",
            "790f944522c44e0e8e8caf5513e402db",
            "c11c3ece0ddf464789c30f04a0e8da4f",
            "9535bf95bc784228bfb62d90f42faba3",
            "fa40fad1ab9e4daf865bcd3d63df4cb0",
            "7af11ea227e1489db47585c027acd418"
          ]
        },
        "id": "05iiYf70lveJ",
        "outputId": "6ae8b5f5-5dd9-48f2-f20f-2f2669c94247"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03a978673c794a728239a0f22fb02efb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: en (English) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06e023a61b2b4a8795d4eb894213a6c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "WARNING:stanza:Language en package default expects mwt, which has been added\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| lemma     | combined_nocharlm |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quick brown fox jump lazy dog chase butterfly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here\n",
        "Stanza Pipeline:\n",
        "The stanza.Pipeline is initialized with the tokenize and lemma processors to tokenize and lemmatize the input sentence.\n",
        "\n",
        "Stopwords Removal:\n",
        "NLTK's English stopwords are used to filter out common words like \"was\", \"in\", etc.\n",
        "\n",
        "Lemmatization:\n",
        "The word.lemma attribute is used to extract the base form of each word.\n",
        "Lemmas are converted to lowercase to ensure consistency.\n",
        "\n",
        "Cleaning:\n",
        "Only alphabetic tokens are retained (using isalpha()), and stopwords are removed.\n",
        "\n",
        "Output:\n",
        "The cleaned and lemmatized words are joined into a single sentence.\n",
        "\n",
        "\n",
        "Input Sentence: \"The quick brown fox jumps over the lazy dog while chasing a butterfly.\"\n",
        "\n",
        "Stopwords Removed: Words like \"the\", \"over\", \"while\", and \"a\" will be removed.\n",
        "\n",
        "Lemmatization: Words like \"jumps\" will be lemmatized to \"jump\", and \"chasing\" will be lemmatized to \"chase\"."
      ],
      "metadata": {
        "id": "Ri0OIiWfBN8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question - 3**\n",
        "\n",
        "(Refer to the byte pair encoding concept which is explained in activity-3 at the Tutorial of Subword Tokenization using HuggingFace after the Question-6 in activity-3)\n",
        "\n",
        "\n",
        "Consider the following two sentences:\n",
        "\n",
        "**S1:** \"The artist is creating a better version of his masterpiece by refining the details.\"\n",
        "\n",
        "**S2:**\"She is bettering her artistic skills by creating new versions of her paintings.\"\n",
        "\n",
        "**Create a Python function that encodes two sentences using the custom BPE tokenizer and identifies common subword tokens (tokens that appear in both encodings). Return a list of these common subword tokens. Is/Are there any interesting observations when you compare the tokens between the two encodings? What do you think is causing what you observe as part of your comparison?**"
      ],
      "metadata": {
        "id": "I7o1E0wTupIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code here\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Define the sentences\n",
        "s1 = \"The artist is creating a better version of his masterpiece by refining the details.\"\n",
        "s2 = \"She is bettering her artistic skills by creating new versions of her paintings.\"\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "# Train the tokenizer on the two sentences\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train_from_iterator([s1, s2], trainer)\n",
        "\n",
        "# Encode the sentences\n",
        "encoding_s1 = tokenizer.encode(s1)\n",
        "encoding_s2 = tokenizer.encode(s2)\n",
        "\n",
        "# Get the tokens for each sentence\n",
        "tokens_s1 = encoding_s1.tokens\n",
        "tokens_s2 = encoding_s2.tokens\n",
        "\n",
        "# Find common subword tokens\n",
        "common_tokens = set(tokens_s1).intersection(set(tokens_s2))\n",
        "\n",
        "# Print results\n",
        "print(\"Tokens for S1:\", tokens_s1)\n",
        "print(\"Tokens for S2:\", tokens_s2)\n",
        "print(\"Common Subword Tokens:\", common_tokens)"
      ],
      "metadata": {
        "id": "3RY65-RPvURG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb0bf3c-f6d1-4859-a61a-be5e23879f58"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens for S1: ['The', 'artist', 'is', 'creating', 'a', 'better', 'version', 'of', 'his', 'masterpiece', 'by', 'refining', 'the', 'details', '.']\n",
            "Tokens for S2: ['She', 'is', 'bettering', 'her', 'artistic', 'skills', 'by', 'creating', 'new', 'versions', 'of', 'her', 'paintings', '.']\n",
            "Common Subword Tokens: {'is', '.', 'by', 'creating', 'of'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task - 4 : Minimum Edit distance (25%)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Minimum edit Distance\n",
        "Minimum Edit Distance (also known as Levenshtein Distance) is a measure of similarity between two strings by calculating the minimum number of single-character edits (insertions, deletions, substitutions) required to transform one string into the other. It has applications in various fields, including natural language processing, spell checking, DNA sequence alignment, and more.\n",
        "\n"
      ],
      "metadata": {
        "id": "IEKqJAYbvUG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character Based Text Similarity\n",
        "\"As an example, this technology is used by information retrieval systems, search engines, automatic indexing systems, text summarizers, categorization systems, plagiarism checkers, speech recognition, rating systems, DNA analysis, and profiling algorithms (IR/AI programs to automatically link data between people and what they do).\""
      ],
      "metadata": {
        "id": "aNZzSXu2xoAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##code block -4\n",
        "# A Naive recursive Python program to find minimum number\n",
        "# operations to convert str1 to str2\n",
        "\n",
        "\n",
        "def editDistance(str1, str2, m, n):\n",
        "\n",
        "    # If first string is empty, the only option is to\n",
        "    # insert all characters of second string into first\n",
        "    if m == 0:\n",
        "        return n\n",
        "\n",
        "    # If second string is empty, the only option is to\n",
        "    # remove all characters of first string\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    # If last characters of two strings are same, nothing\n",
        "    # much to do. Ignore last characters and get count for\n",
        "    # remaining strings.\n",
        "    if str1[m-1] == str2[n-1]:\n",
        "        return editDistance(str1, str2, m-1, n-1)\n",
        "\n",
        "    # If last characters are not same, consider all three\n",
        "    # operations on last character of first string, recursively\n",
        "    # compute minimum cost for all three operations and take\n",
        "    # minimum of three values.\n",
        "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert\n",
        "                   editDistance(str1, str2, m-1, n),    # Remove\n",
        "                   editDistance(str1, str2, m-1, n-1)    # Replace\n",
        "                   )\n",
        "\n",
        "\n",
        "# Driver code\n",
        "str1 = \"sunday\"\n",
        "str2 = \"saturday\"\n",
        "print (editDistance(str1, str2, len(str1), len(str2)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHA_ccV_xmCB",
        "outputId": "0f8944cb-413d-4740-dd8e-5ba112fbb6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer above code block -4  for the following question"
      ],
      "metadata": {
        "id": "nSfpOff4Ngbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-1\n",
        "##Assuming case sensitivity where changing a letter's case has a cost of 1, calculate the minimum cost to transform \"Educate\" into \"Education\" with the following operation costs: insertions = 3, deletions = 1, substitutions = 2"
      ],
      "metadata": {
        "id": "OUJ1n35-dgON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##your code here\n",
        "def min_edit_distance(str1, str2, insert_cost=3, delete_cost=1, substitute_cost=2, case_change_cost=1):\n",
        "    m, n = len(str1), len(str2)\n",
        "    # Create a DP table to store edit distances\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the first row and column\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i * delete_cost  # Cost of deleting all characters from str1\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j * insert_cost  # Cost of inserting all characters of str2\n",
        "\n",
        "    # Fill the DP table\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                # Characters are the same, no cost\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            elif str1[i - 1].lower() == str2[j - 1].lower():\n",
        "                # Characters are the same but differ in case\n",
        "                dp[i][j] = dp[i - 1][j - 1] + case_change_cost\n",
        "            else:\n",
        "                # Characters are different, consider all operations\n",
        "                dp[i][j] = min(\n",
        "                    dp[i][j - 1] + insert_cost,      # Insert\n",
        "                    dp[i - 1][j] + delete_cost,      # Delete\n",
        "                    dp[i - 1][j - 1] + substitute_cost  # Substitute\n",
        "                )\n",
        "    return dp[m][n]\n",
        "\n",
        "# Input strings\n",
        "str1 = \"Educate\"\n",
        "str2 = \"Education\"\n",
        "\n",
        "# Calculate minimum edit distance\n",
        "cost = min_edit_distance(str1, str2)\n",
        "print(f\"Minimum edit distance: {cost}\")"
      ],
      "metadata": {
        "id": "bt4ZjDL2c3LB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c3c361-36fe-42a4-bab9-5d832bafe826"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum edit distance: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation\n",
        "Custom Costs:\n",
        "\n",
        "Insertion cost = 3\n",
        "\n",
        "Deletion cost = 1\n",
        "\n",
        "Substitution cost = 2\n",
        "\n",
        "Dynamic Programming Table (dp):\n",
        "\n",
        "dp[i][j] represents the minimum cost to transform the first i characters in str1 into the first j characters of str2.\n",
        "\n",
        "Cost Calculation:\n",
        "\n",
        "Insert 'i': Cost = 3\n",
        "\n",
        "Insert 'o': Cost = 3\n",
        "\n",
        "Total cost = 3 + 3 = 6\n",
        "\n",
        "Operations:\n",
        "Insert 'i' and 'o' at the end of \"Educate\" to match \"Education\".\n",
        "No deletions or substitutions are needed\n",
        "\n",
        "And If the characters are completely different, minimum cost for insertion, deletion, and substitution is selected."
      ],
      "metadata": {
        "id": "6fdCkNgZdL7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tutorial -2\n",
        "# Levenshtein Distance for Sentences"
      ],
      "metadata": {
        "id": "RFiLMn7rIHUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code block - 5\n",
        "# Simple Minimum Edit Distance\n",
        "def levenshtein_distance(str1, str2):\n",
        "    # Initialize a matrix to store edit distances\n",
        "    m, n = len(str1), len(str2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the first row and column\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    # Fill in the matrix using dynamic programming\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 2  # Substitution cost is 2\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,  # Deletion\n",
        "                dp[i][j - 1] + 1,  # Insertion\n",
        "                dp[i - 1][j - 1] + cost,  # Substitution\n",
        "            )\n",
        "\n",
        "    # The final value in the matrix represents the Levenshtein distance\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "str1 = \"This is a cat\"\n",
        "str2 = \"That is a dog\"\n",
        "distance = levenshtein_distance(str1, str2)\n",
        "print(f\"The Levenshtein distance between '{str1}' and '{str2}' with substitution cost 2 is {distance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FypLc-dkO96G",
        "outputId": "3fed1b62-794c-4941-c9f2-8311285def2c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Levenshtein distance between 'This is a cat' and 'That is a dog' with substitution cost 2 is 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to above code block -5 from tutorial - 2 for the following question"
      ],
      "metadata": {
        "id": "e-xrzKVC5iSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question:2\n",
        "##Assign different costs to insertions, deletions, and substitutions to reflect varying penalties for different types of edits. Calculate the Levenshtein distance with these weighted costs.\n",
        "\n",
        "String1 = (\"Natural language processing\")\n",
        "\n",
        "String2 = (\"Computer science department\")\n",
        "\n",
        "Provide your explanation in the tex block below"
      ],
      "metadata": {
        "id": "EapfLAq5iCLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##ENTER YOUR CODE HERE\n",
        "def weighted_levenshtein_distance(str1, str2, insert_cost=1, delete_cost=1, substitute_cost=2):\n",
        "    m, n = len(str1), len(str2)\n",
        "    # Initialize a DP table to store edit distances\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the first row and column\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i * delete_cost  # Cost of deleting all characters from str1\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j * insert_cost  # Cost of inserting all characters of str2\n",
        "\n",
        "    # Fill the DP table\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                # Characters are the same, no cost\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                # Characters are different, consider all operations\n",
        "                dp[i][j] = min(\n",
        "                    dp[i][j - 1] + insert_cost,      # Insert\n",
        "                    dp[i - 1][j] + delete_cost,      # Delete\n",
        "                    dp[i - 1][j - 1] + substitute_cost  # Substitute\n",
        "                )\n",
        "    return dp[m][n]\n",
        "\n",
        "# Input strings\n",
        "str1 = \"Natural language processing\"\n",
        "str2 = \"Computer science department\"\n",
        "\n",
        "# Calculate weighted Levenshtein distance\n",
        "distance = weighted_levenshtein_distance(str1, str2, insert_cost=1, delete_cost=1, substitute_cost=2)\n",
        "print(f\"Weighted Levenshtein distance between '{str1}' and '{str2}' is {distance}\")"
      ],
      "metadata": {
        "id": "kcDvlYTD4YvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4f22b7-773e-4869-b5d4-321f145e5ad5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Levenshtein distance between 'Natural language processing' and 'Computer science department' is 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation\n",
        "Alignment:\n",
        "Natural language processing\n",
        "Computer science department\n",
        "\n",
        "Operations:\n",
        "\n",
        "Substitute 'N' with 'C': Cost = 2\n",
        "\n",
        "Substitute 'a' with 'o': Cost = 2\n",
        "\n",
        "Substitute 't' with 'm': Cost = 2\n",
        "\n",
        "Substitute 'u' with 'p': Cost = 2\n",
        "\n",
        "Substitute 'r' with 'u': Cost = 2\n",
        "\n",
        "Substitute 'a' with 't': Cost = 2\n",
        "\n",
        "Substitute 'l' with 'e': Cost = 2\n",
        "\n",
        "Substitute ' ' with 'r': Cost = 2\n",
        "\n",
        "Substitute 'l' with ' ': Cost = 2\n",
        "\n",
        "Substitute 'a' with 's': Cost = 2\n",
        "\n",
        "Substitute 'n' with 'c': Cost = 2\n",
        "\n",
        "Substitute 'g' with 'i': Cost = 2\n",
        "\n",
        "Substitute 'u' with 'e': Cost = 2\n",
        "\n",
        "Substitute 'a' with 'n': Cost = 2\n",
        "\n",
        "Substitute 'g' with 'c': Cost = 2\n",
        "\n",
        "Substitute 'e' with 'e': Cost = 0 (no change)\n",
        "\n",
        "Substitute ' ' with ' ': Cost = 0 (no change)\n",
        "\n",
        "Substitute 'p' with 'd': Cost = 2\n",
        "\n",
        "Substitute 'r' with 'e': Cost = 2\n",
        "\n",
        "Substitute 'o' with 'p': Cost = 2\n",
        "\n",
        "Substitute 'c' with 'a': Cost = 2\n",
        "\n",
        "Substitute 'e' with 'r': Cost = 2\n",
        "\n",
        "Substitute 's' with 't': Cost = 2\n",
        "\n",
        "Substitute 's' with 'm': Cost = 2\n",
        "\n",
        "Substitute 'i' with 'e': Cost = 2\n",
        "\n",
        "Substitute 'n' with 'n': Cost = 0 (no change)\n",
        "\n",
        "Substitute 'g' with 't': Cost = 2\n",
        "\n",
        "Total Cost:\n",
        "\n",
        "Total substitutions = 22\n",
        "\n",
        "Total cost = 22 * 2 = 44\n"
      ],
      "metadata": {
        "id": "texg77AFdG6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question -3\n",
        "\n",
        "##Modify the minimum edit distance algorithm to account for different penalties:\n",
        "\n",
        "Insertion = 1\n",
        "\n",
        "Deletion = 2\n",
        "\n",
        "Substitution = 3\n",
        "\n",
        "Apply it to the following sentences:\n",
        "\n",
        "**Senetence 1 = \"Data Science is evolving fast.**\"\n",
        "\n",
        "**Senetence 2 = \"Artificial Intelligence is transforming industries.**\"\n",
        "\n",
        "\n",
        "Explain how different weights reflect real-world penalties in tasks like document comparison or DNA sequence matching."
      ],
      "metadata": {
        "id": "pg7XS-bAztur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code here\n",
        "def weighted_min_edit_distance(str1, str2, insert_cost=2, delete_cost=3, substitute_cost=5):\n",
        "    m, n = len(str1), len(str2)\n",
        "    # Initialize a DP table to store edit distances\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the first row and column\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i * delete_cost  # Cost of deleting all characters from str1\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j * insert_cost  # Cost of inserting all characters of str2\n",
        "\n",
        "    # Fill the DP table\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                # Characters are the same, no cost\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                # Characters are different, consider all operations\n",
        "                dp[i][j] = min(\n",
        "                    dp[i][j - 1] + insert_cost,      # Insert\n",
        "                    dp[i - 1][j] + delete_cost,      # Delete\n",
        "                    dp[i - 1][j - 1] + substitute_cost  # Substitute\n",
        "                )\n",
        "    return dp[m][n]\n",
        "\n",
        "# Input sentences\n",
        "str1 = \"Data Science is evolving fast.\"\n",
        "str2 = \"Artificial Intelligence is transforming industries.\"\n",
        "\n",
        "# Calculate weighted minimum edit distance\n",
        "distance = weighted_min_edit_distance(str1, str2, insert_cost=1, delete_cost=2, substitute_cost=3)\n",
        "print(f\"Weighted Minimum Edit Distance between '{str1}' and '{str2}' is {distance}\")"
      ],
      "metadata": {
        "id": "xjRSiKhj0avx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31498311-7603-4052-bd19-a2e718c42781"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Minimum Edit Distance between 'Data Science is evolving fast.' and 'Artificial Intelligence is transforming industries.' is 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-4\n",
        "##Explain how Minimum Edit Distance (MED) is applied in NLP tasks like spell checking, speech recognition, text summarization, and machine translation. How does its effectiveness differ across these tasks?"
      ],
      "metadata": {
        "id": "8zLdz7KC0YbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your Explanation\n",
        "Minimum Edit Distance is a method used in NLP to measure how different two strings are by counting the number of edits like insertions, deletions and substitutions suppose  to convert one into another. It‚Äôs very efficient in many tasks:\n",
        "\n",
        "Spell Checking: MED helps find the closest valid word to a misspelled/wrong word. As if a user types \"recieve,\" MED compares it to valid words like \"receive\" and suggests the closest match.\n",
        "\n",
        "Speech Recognition: In speech-to-text, It helps compare the predicted transcript with the real words. It measures how many corrections are needed to fix errors like wrong words or missing letters.\n",
        "\n",
        "Text Summarization: MED can evaluate the quality of generated summaries by matching them to reference summaries. Fewer edits mean the summary is closer to the real one.\n",
        "\n",
        "Machine Translation: MED is used to evaluate translation quality by comparing machine outputs to human translations. It shows how many changes are required to make the machine translation correct.\n",
        "\n",
        "Effectiveness depends on the task. For instance, in spell checking, It works well because edits are usually small. But for machine translation, MED might not capture the meaning or grammar differences fully, so other metrics are often needed."
      ],
      "metadata": {
        "id": "jfyAF_SacJZd"
      }
    }
  ]
}